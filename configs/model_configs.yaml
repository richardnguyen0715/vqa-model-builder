data:
  batch_size: 32
  num_workers: 4
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1
  image_size: [224, 224]


model_config:
  model_name: "autovivqa_model"
  pretrained_model_path: "pretrained_models/autovivqa_pretrained.pth"  
  embedding_dim: 300
  hidden_dim: 512
  num_layers: 2
  dropout: 0.3

tokenizer_config:
  model_name: "bert-base-uncased"
  max_len: 30

coco_normalize:
  mean: [0.470, 0.446, 0.408]
  std: [0.277, 0.274, 0.288]

imagenet_normalize:
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]

training_config:
  num_epochs: 50
  learning_rate: 0.001
  weight_decay: 0.0001
  lr_scheduler_step_size: 10
  lr_scheduler_gamma: 0.1
  early_stopping_patience: 5
  gradient_clip_value: 5.0