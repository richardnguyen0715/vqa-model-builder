# ğŸ‡»ğŸ‡³ AutoViVQA Model Builder

<div align="center">

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                                          â•‘
â•‘  â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—     â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â•‘
â•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•  â•‘
â•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â•‘
â•‘  â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–„â–„ â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•    â•‘
â•‘   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â•‘
â•‘    â•šâ•â•â•â•   â•šâ•â•â–€â–€â•â• â•šâ•â•  â•šâ•â•    â•šâ•â•     â•šâ•â•â•šâ•â•     â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â•  â•‘
â•‘                                                                                          â•‘
â•‘                            Vietnamese Visual Question Answering                          â•‘
â•‘                                  AutoViVQA Model Builder                                 â•‘
â•‘                                                                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.9+-ee4c2c.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

**A comprehensive Vietnamese Visual Question Answering system with state-of-the-art features**

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Quick Start](#-quick-start) â€¢ [Documentation](#-documentation) â€¢ [Architecture](#-architecture)

</div>

---

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Architecture Overview](#-architecture-overview)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Pipeline Usage](#-pipeline-usage)
  - [Training](#training)
  - [Evaluation](#evaluation)
  - [Inference](#inference)
- [Configuration](#-configuration)
- [Project Structure](#-project-structure)
- [Documentation](#-documentation)
- [Development](#-development)
- [License](#-license)

---

## âœ¨ Features

### ğŸ¯ Core Capabilities

| Feature | Description |
|---------|-------------|
| **Vietnamese NLP** | Optimized for Vietnamese with PhoBERT integration |
| **Multiple Visual Backbones** | ViT, ResNet, CLIP, Swin Transformer |
| **Multimodal Fusion** | Cross-Attention, Q-Former, Bilinear fusion strategies |
| **Mixture of Experts (MOE)** | Dynamic routing to specialized expert networks |
| **Knowledge Base/RAG** | Retrieval-Augmented Generation for external knowledge |
| **Memory-Efficient** | Lazy image loading, AMP training |

### ğŸ”§ Technical Features

- âœ… **End-to-end Pipeline** - Data loading â†’ Model building â†’ Training â†’ Evaluation
- âœ… **Comprehensive Logging** - Detailed validation at every step
- âœ… **Resource Monitoring** - Real-time CPU, GPU, Memory tracking
- âœ… **Checkpointing** - Auto-save best models with early stopping
- âœ… **Mixed Precision** - FP16/BF16 training for faster performance
- âœ… **CLI & Python API** - Flexible usage options

---

## ğŸ— Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Vietnamese VQA Model                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Image      â”‚     â”‚   Question   â”‚     â”‚  Knowledge   â”‚    â”‚
â”‚  â”‚   Input      â”‚     â”‚   (Vietnamese)â”‚     â”‚    Base      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                    â”‚                    â”‚              â”‚
â”‚         â–¼                    â–¼                    â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Visual     â”‚     â”‚    Text      â”‚     â”‚     RAG      â”‚    â”‚
â”‚  â”‚   Encoder    â”‚     â”‚   Encoder    â”‚     â”‚   Module     â”‚    â”‚
â”‚  â”‚  (ViT/CLIP)  â”‚     â”‚  (PhoBERT)   â”‚     â”‚  (Retriever) â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                    â”‚                    â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                      â–¼                                          â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚              â”‚     Multimodal Fusion            â”‚               â”‚
â”‚              â”‚  (Cross-Attention / Q-Former)    â”‚               â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                             â–¼                                    â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚              â”‚    Mixture of Experts (MOE)      â”‚               â”‚
â”‚              â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”   â”‚               â”‚
â”‚              â”‚  â”‚ E1 â”‚ â”‚ E2 â”‚ â”‚ E3 â”‚ â”‚ E4 â”‚   â”‚               â”‚
â”‚              â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜   â”‚               â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                             â–¼                                    â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚              â”‚        Answer Head               â”‚               â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                             â–¼                                    â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚                      â”‚  Answer  â”‚                               â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» Installation

### Prerequisites

- Python 3.11+
- CUDA 12.x (for GPU support)
- 8GB+ RAM (16GB+ recommended)
- NVIDIA GPU with 8GB+ VRAM (optional but recommended)

### Step 1: Clone Repository

```bash
git clone https://github.com/yourusername/AutovivqaModelBuilder.git
cd AutovivqaModelBuilder
```

### Step 2: Create Environment

```bash
# Using conda (recommended)
conda create -n vqa python=3.11
conda activate vqa

# Or using venv
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows
```

### Step 3: Install Dependencies

```bash
# Using poetry (recommended)
pip install poetry
poetry install

# Or using pip
pip install -r requirements.txt
```

### Step 4: Download Data

```bash
# Using CLI script
bash src/cli/download_data.sh

# Or using Python
python -m src.data.download_data
```

---

## ğŸš€ Quick Start

### Option 1: One-Command Training

```bash
# Download data + Train model (recommended for first time)
bash src/cli/quick_start.sh
```

### Option 2: CLI Training

```bash
# Basic training
python -m src.core.vqa_pipeline --mode train --epochs 10 --batch-size 16

# With all options
python -m src.core.vqa_pipeline \
    --mode train \
    --epochs 20 \
    --batch-size 32 \
    --learning-rate 2e-5 \
    --visual-backbone vit \
    --text-encoder phobert \
    --use-moe \
    --output-dir outputs
```

### Option 3: Using Config File

```bash
# Use YAML configuration
python -m src.core.vqa_pipeline --config configs/pipeline_config.yaml
```

### Option 4: Shell Script

```bash
# Using shell script with arguments
bash src/cli/run_pipeline.sh --mode train --epochs 10 --batch-size 16

# Or use clean mode (suppresses warnings)
bash src/cli/run_clean.sh --mode train --epochs 10
```

### Option 5: Python API

```python
from src.core import VQAPipeline, VQAPipelineConfig
from src.core import DataPipelineConfig, ModelPipelineConfig, TrainingPipelineConfig

# Configure pipeline
config = VQAPipelineConfig(
    mode="train",
    data=DataPipelineConfig(
        images_dir="data/raw/images",
        batch_size=16,
    ),
    model=ModelPipelineConfig(
        visual_backbone="vit",
        text_encoder_type="phobert",
        use_moe=False,
    ),
    training=TrainingPipelineConfig(
        num_epochs=10,
        learning_rate=2e-5,
    ),
)

# Run pipeline
pipeline = VQAPipeline(config)
results = pipeline.run()

print(f"Best accuracy: {results.training_output.best_metric:.4f}")
```

---

## ğŸ“– Pipeline Usage

### Training

#### Basic Training

```bash
python -m src.core.vqa_pipeline --mode train --epochs 10
```

#### Advanced Training with MOE

```bash
python -m src.core.vqa_pipeline \
    --mode train \
    --epochs 20 \
    --batch-size 32 \
    --use-moe \
    --learning-rate 2e-5 \
    --output-dir outputs/moe_experiment
```

#### Training with Knowledge Base (RAG)

```bash
python -m src.core.vqa_pipeline \
    --mode train \
    --epochs 20 \
    --use-knowledge \
    --output-dir outputs/rag_experiment
```

#### Resume Training from Checkpoint

```bash
python -m src.core.vqa_pipeline \
    --mode train \
    --resume checkpoints/checkpoint_epoch_5.pt
```

### Evaluation

```bash
# Evaluate on test set
python -m src.core.vqa_pipeline --mode evaluate --resume checkpoints/best_model.pt
```

### Inference

```bash
# Run inference on new images
python -m src.core.vqa_pipeline --mode inference --resume checkpoints/best_model.pt
```

---

## âš™ï¸ Configuration

### Configuration Files

| File | Description |
|------|-------------|
| `configs/pipeline_config.yaml` | Complete pipeline configuration |
| `configs/model_configs.yaml` | Model architecture settings |
| `configs/training_configs.yaml` | Training hyperparameters |
| `configs/data_configs.yaml` | Data loading settings |
| `configs/resource_configs.yaml` | Resource monitoring thresholds |

### Key Configuration Options

```yaml
# configs/pipeline_config.yaml

# Data Configuration
data:
  images_dir: data/raw/images
  batch_size: 32
  num_workers: 4
  augmentation_strength: medium  # light, medium, strong

# Model Configuration
model:
  visual_backbone: vit           # vit, resnet, clip, swin
  text_encoder_type: phobert     # phobert, bert
  fusion_type: cross_attention   # cross_attention, concat, bilinear
  use_moe: false                 # Enable Mixture of Experts
  use_knowledge: false           # Enable Knowledge Base/RAG

# Training Configuration
training:
  num_epochs: 20
  learning_rate: 2.0e-5
  optimizer_name: adamw
  scheduler_name: cosine
  use_amp: true                  # Mixed precision training
  early_stopping: true
  patience: 5
```

### CLI Arguments Reference

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--mode` | str | train | Pipeline mode: train, evaluate, inference |
| `--config` | str | None | Path to YAML config file |
| `--epochs` | int | 20 | Number of training epochs |
| `--batch-size` | int | 32 | Training batch size |
| `--learning-rate` | float | 2e-5 | Learning rate |
| `--visual-backbone` | str | vit | Visual encoder: vit, resnet, clip, swin |
| `--text-encoder` | str | phobert | Text encoder: phobert, bert |
| `--use-moe` | flag | False | Enable Mixture of Experts |
| `--use-knowledge` | flag | False | Enable Knowledge Base/RAG |
| `--output-dir` | str | outputs | Output directory |
| `--resume` | str | None | Resume from checkpoint |

---

## ğŸ“ Project Structure

```
AutovivqaModelBuilder/
â”œâ”€â”€ ğŸ“ configs/                    # Configuration files
â”‚   â”œâ”€â”€ pipeline_config.yaml       # Main pipeline config
â”‚   â”œâ”€â”€ model_configs.yaml         # Model architecture
â”‚   â”œâ”€â”€ training_configs.yaml      # Training settings
â”‚   â”œâ”€â”€ data_configs.yaml          # Data loading
â”‚   â””â”€â”€ resource_configs.yaml      # Resource monitoring
â”‚
â”œâ”€â”€ ğŸ“ src/                        # Source code
â”‚   â”œâ”€â”€ ğŸ“ core/                   # Core pipeline modules
â”‚   â”‚   â”œâ”€â”€ vqa_pipeline.py        # Main orchestrator
â”‚   â”‚   â”œâ”€â”€ data_pipeline.py       # Data loading pipeline
â”‚   â”‚   â”œâ”€â”€ model_pipeline.py      # Model building pipeline
â”‚   â”‚   â”œâ”€â”€ training_pipeline.py   # Training loop pipeline
â”‚   â”‚   â””â”€â”€ pipeline_logger.py     # Comprehensive logging
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ cli/                    # Command-line interface
â”‚   â”‚   â”œâ”€â”€ run_pipeline.sh        # Main CLI script
â”‚   â”‚   â”œâ”€â”€ quick_start.sh         # Quick start script
â”‚   â”‚   â”œâ”€â”€ run_clean.sh           # Clean output script
â”‚   â”‚   â””â”€â”€ download_data.sh       # Data download script
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ data/                   # Data handling
â”‚   â”‚   â”œâ”€â”€ data_actions.py        # Data loading functions
â”‚   â”‚   â”œâ”€â”€ dataset.py             # PyTorch Dataset classes
â”‚   â”‚   â”œâ”€â”€ augmentation.py        # Image augmentation
â”‚   â”‚   â””â”€â”€ download_data.py       # Kaggle data download
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ modeling/               # Model architecture
â”‚   â”‚   â”œâ”€â”€ ğŸ“ meta_arch/          # Main VQA model
â”‚   â”‚   â”‚   â”œâ”€â”€ vqa_model.py       # VietnameseVQAModel
â”‚   â”‚   â”‚   â””â”€â”€ vqa_config.py      # Model configurations
â”‚   â”‚   â”œâ”€â”€ ğŸ“ backbone/           # Visual encoders
â”‚   â”‚   â”œâ”€â”€ ğŸ“ fusion/             # Multimodal fusion
â”‚   â”‚   â”œâ”€â”€ ğŸ“ moe/                # Mixture of Experts
â”‚   â”‚   â”œâ”€â”€ ğŸ“ knowledge_base/     # RAG module
â”‚   â”‚   â””â”€â”€ ğŸ“ tokenizer/          # Text tokenizers
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ pipeline/               # Training utilities
â”‚   â”‚   â”œâ”€â”€ ğŸ“ trainer/            # Training loops
â”‚   â”‚   â””â”€â”€ ğŸ“ evaluator/          # Evaluation metrics
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ resource_management/    # Resource monitoring
â”‚   â”‚   â”œâ”€â”€ resource_monitor.py    # CPU/GPU/Memory monitor
â”‚   â”‚   â””â”€â”€ resource_manager.py    # Auto-backup on thresholds
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ middleware/             # Utilities
â”‚       â”œâ”€â”€ config_loader.py       # Config loading
â”‚       â”œâ”€â”€ logger.py              # Logging setup
â”‚       â””â”€â”€ monitor.py             # Memory monitoring
â”‚
â”œâ”€â”€ ğŸ“ data/                       # Data directory
â”‚   â””â”€â”€ ğŸ“ raw/                    # Raw data
â”‚       â”œâ”€â”€ ğŸ“ images/             # Image files
â”‚       â””â”€â”€ ğŸ“ texts/              # CSV/JSON annotations
â”‚
â”œâ”€â”€ ğŸ“ docs/                       # Documentation
â”‚   â”œâ”€â”€ vqa_architecture.md        # Architecture details
â”‚   â”œâ”€â”€ fusion_approaches.md       # Fusion strategies
â”‚   â”œâ”€â”€ moe_approaches.md          # MOE documentation
â”‚   â””â”€â”€ prepare_data.md            # Data preparation guide
â”‚
â”œâ”€â”€ ğŸ“ checkpoints/                # Model checkpoints
â”œâ”€â”€ ğŸ“ outputs/                    # Training outputs
â”œâ”€â”€ ğŸ“ logs/                       # Log files
â”‚
â”œâ”€â”€ pyproject.toml                 # Project dependencies
â”œâ”€â”€ README.md                      # This file
â””â”€â”€ LICENSE                        # MIT License
```

---

## ğŸ“š Documentation

### Core Documentation

| Document | Description |
|----------|-------------|
| [VQA Architecture](docs/vqa_architecture.md) | Complete system architecture |
| [Fusion Approaches](docs/fusion_approaches.md) | Multimodal fusion strategies |
| [MOE Approaches](docs/moe_approaches.md) | Mixture of Experts module |
| [Knowledge Base](docs/knowledge_base_approaches.md) | RAG integration |
| [Data Preparation](docs/prepare_data.md) | Data setup guide |

### Pipeline Output

After training, the pipeline generates:

```
outputs/
â”œâ”€â”€ pipeline_summary.json       # Complete training summary
â”œâ”€â”€ training_curves.png         # Loss/accuracy plots
â””â”€â”€ predictions/                # Model predictions

checkpoints/
â”œâ”€â”€ best_model.pt               # Best model checkpoint
â”œâ”€â”€ checkpoint_epoch_1.pt       # Epoch checkpoints
â””â”€â”€ checkpoint_epoch_N.pt

logs/
â”œâ”€â”€ tensorboard/                # TensorBoard logs
â””â”€â”€ pipeline/                   # Pipeline logs
```

### Logging Output Example

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                   VQA PIPELINE                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

================================================================================
============================= VQA PIPELINE STARTED =============================
================================================================================
    Mode: train
    Output directory: outputs
    Start time: 2026-01-10 00:01:28

-------------------- System Information --------------------
    Platform: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
    Python version: 3.11.14
    PyTorch version: 2.9.1+cu128
    CUDA available: True
    GPU 0: NVIDIA GeForce RTX 3060 (11.6 GB)
    Total RAM: 31.1 GB

================================================================================
============================ STAGE 1: DATA PIPELINE ============================
================================================================================
âœ“ Loaded 37077 data samples
âœ“ Data split: 29661 train / 3707 val / 3709 test
âœ“ Built vocabulary with 447 answer classes
âœ“ DATA PIPELINE completed in 7.22s

================================================================================
=========================== STAGE 2: MODEL PIPELINE ============================
================================================================================
âœ“ Using CUDA: NVIDIA GeForce RTX 3060
    Total parameters: 243,163,583
    Trainable parameters: 243,163,583
    Model size (MB): 927.60
âœ“ MODEL PIPELINE completed in 6.97s

================================================================================
========================== STAGE 3: TRAINING PIPELINE ==========================
================================================================================
Epoch 1 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1853/1853 [05:10<00:00, 5.97it/s, loss=1.88, acc=0.76]
Epoch 1 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:13<00:00, 4.23it/s, loss=0.35, acc=0.96]
âœ“ New best accuracy: 0.9625
âœ“ Checkpoint saved [BEST]: checkpoints/best_model.pt

================================================================================
=============================== PIPELINE SUMMARY ===============================
================================================================================
    Status: SUCCESS
    Total execution time: 357.44s (6.0 min)
    Best metric: 0.9625
    Best model path: checkpoints/best_model.pt
```

---

## ï¿½ Documentation

Comprehensive documentation is available in the `docs/` directory:

### Getting Started
| Document | Description |
|----------|-------------|
| [Getting Started Guide](docs/getting_started.md) | Installation and first run |
| [Pipeline Usage](docs/pipeline_usage.md) | Complete pipeline documentation |
| [Configuration Guide](docs/configuration_guide.md) | All configuration options |
| [API Reference](docs/api_reference.md) | Python API documentation |

### Architecture & Design
| Document | Description |
|----------|-------------|
| [VQA Architecture](docs/vqa_architecture.md) | System architecture overview |
| [Fusion Approaches](docs/fusion_approaches.md) | Multimodal fusion strategies |
| [MOE Approaches](docs/moe_approaches.md) | Mixture of Experts design |
| [Knowledge Base](docs/knowledge_base_approaches.md) | RAG implementation |

### Component Documentation
| Document | Description |
|----------|-------------|
| [Image Representation](docs/image_representation_approaches.md) | Visual encoders |
| [Text Representation](docs/text_representation_approaches.md) | Text encoders |
| [Data Preparation](docs/prepare_data.md) | Dataset preparation |

---

## ï¿½ğŸ”¬ Development

### Running Tests

```bash
# Run all tests
pytest tests/

# Run specific test
pytest tests/test_pipeline.py -v
```

### Code Quality

```bash
# Format code
black src/

# Lint code
flake8 src/

# Type checking
mypy src/
```

### TensorBoard

```bash
# Launch TensorBoard
tensorboard --logdir logs/tensorboard

# Open in browser: http://localhost:6006
```

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Tuong Nguyen**
- Email: richardnguyen0715@gmail.com
- GitHub: [@tuong.nguyen](https://github.com/tuong.nguyen)

---

## ğŸ™ Acknowledgments

- [VinAI Research](https://vinai.io/) for PhoBERT
- [OpenAI](https://openai.com/) for CLIP
- [Hugging Face](https://huggingface.co/) for Transformers
- [PyTorch](https://pytorch.org/) team

---

<div align="center">

**â­ Star this repo if you find it helpful!**

Made with â¤ï¸ for Vietnamese NLP

</div>
